# Auto-generated by no-code generator. Do not edit by hand.
from __future__ import annotations
from datetime import timedelta
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
import pandas as pd
import os
import tempfile
    import s3fs
from airflow.hooks.base import BaseHook
from sqlalchemy import create_engine

CONFIG = {{ config_json }}

default_args = {
    "retries": {{ retries }},
}

def extract(**context):
    src = CONFIG["source"]
    tempdir = tempfile.mkdtemp(prefix="{{ dag_id }}_")
    out_path = os.path.join(tempdir, "extract.parquet")
    if src["type"] == "local_csv":
        df = pd.read_csv(src["path"], **(src.get("options") or {}))
    else:
        raise ValueError(f"Unsupported source type: {src['type']}")
    df.to_parquet(out_path, index=False)
    context['ti'].xcom_push(key='dataset_path', value=out_path)

def apply_transforms(df: pd.DataFrame) -> pd.DataFrame:
    for step in CONFIG.get("transforms", []):
        t = step["type"]
        if t == "select":
            df = df[step["columns"]]
        elif t == "cast":
            for col, typ in step["columns"].items():
                if typ == "int":
                    df[col] = pd.to_numeric(df[col], errors="raise").astype("Int64").astype("int64")
                elif typ == "float":
                    df[col] = pd.to_numeric(df[col], errors="raise")
                elif typ == "timestamp":
                    df[col] = pd.to_datetime(df[col], errors="raise", utc=False)
                elif typ == "string":
                    df[col] = df[col].astype("string")
                else:
                    raise ValueError(f"Unsupported cast type: {typ}")
        elif t == "filter":
            expr = step["expr"]
            df = df.query(expr)
        else:
            raise ValueError(f"Unsupported transform type: {t}")
    return df

def quality(**context):
    path = context['ti'].xcom_pull(key='dataset_path')
    df = pd.read_parquet(path)
    for check in CONFIG.get("quality_checks", []):
        t = check["type"]
        if t == "not_null":
            col = check["column"]
            if df[col].isna().any():
                raise ValueError(f"Nulls found in {col}")
        elif t == "unique":
            cols = check["columns"]
            dupes = df.duplicated(subset=cols)
            if dupes.any():
                raise ValueError(f"Duplicates found for columns {cols}")
        elif t == "row_count_min":
            mn = check["min"]
            if len(df) < mn:
                raise ValueError(f"Row count {len(df)} < min {mn}")
        else:
            raise ValueError(f"Unsupported quality check: {t}")

def transform(**context):
    path = context['ti'].xcom_pull(key='dataset_path')
    df = pd.read_parquet(path)
    df = apply_transforms(df)
    # overwrite
    df.to_parquet(path, index=False)

def load(**context):
    tgt = CONFIG["target"]
    path = context['ti'].xcom_pull(key='dataset_path')
    df = pd.read_parquet(path)
    if tgt["type"] == "postgres":
        conn = BaseHook.get_connection(tgt["conn_id"])
        db = conn.schema or ""
        url = f"postgresql+psycopg2://{conn.login}:{conn.password}@{conn.host}:{conn.port}/{db}"
        engine = create_engine(url)
        if_exists = "append" if (tgt.get("mode") or "append") == "append" else "replace"
        df.to_sql(tgt["table"], engine, if_exists=if_exists, index=False, chunksize=tgt.get("batch_size") or 10000)
    else:
        raise ValueError(f"Unsupported target type: {tgt['type']}")

with DAG(
    dag_id="{{ dag_id }}",
    default_args=default_args,
    schedule_interval="{{ schedule }}",
    start_date=days_ago(1),
    catchup=False,
    tags={{ tags }},
) as dag:

    extract_task = PythonOperator(task_id="extract", python_callable=extract)
    transform_task = PythonOperator(task_id="transform", python_callable=transform)
    quality_task = PythonOperator(task_id="quality", python_callable=quality)
    load_task = PythonOperator(task_id="load", python_callable=load)

    extract_task >> transform_task >> quality_task >> load_task
